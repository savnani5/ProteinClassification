# Configuration file for training

training:
  seq_max_len: 120
  gpu: 1
  epochs: 5 
  batch_size: 4
  num_workers: 0
  optimizer:
    lr: 0.01 
    momentum: 0.9 
    weight_decay: 0.01
  lr_scheduler:
    milestones: [5, 8, 10, 12, 14, 16, 18, 20]
    gamma: 0.95

model:
  type: 'Transformer' 
  seq_max_len: 120
  n_head: 8 
  n_layers: 6 
  linear_input_channels: 2640 